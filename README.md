# MECO â€“ Micro-Evolutionary Code Optimizer

MECO generates multiple LLM-written variants of a Python function, executes them in a sandboxed container, and performs forward-selection to keep the fastest/leanest candidate. It is packaged for installation and ready for OpenAI's API.

## Features
- LLM-generated code variants (three per step) via OpenAI.
- Deterministic metrics collection inside Docker (runtime, CPU time, RSS memory).
- Forward-selection search with depth/epsilon stopping.
- Automatic unittest generation when none are supplied.
- `src/` layout for installable packaging plus ready-to-run examples.

## Project Layout
```
src/meco/           # Package code
  __init__.py
  code_optimizer.py
  models.py
  workflow.py
  pyparser.py
examples/           # Usage samples
```

## Prerequisites
- Python 3.9+
- Docker daemon running locally
- Environment variable: `OPENAI_API_KEY`

## Installation
```bash
pip install -e .
```

## Quickstart
```bash
python examples/factorial.py
```

Or integrate directly:
```python
from meco.workflow import Workflow

code = """
def factorial(n: int) -> int:
    if n == 0 or n == 1:
        return 1
    return n * factorial(n - 1)
"""

meco = Workflow()
optimized_code, metrics = meco.iterate(
    num_iterations=3,
    function=code,
)

print(optimized_code)
print(metrics)
```

## Configuration Knobs
- `RUNS_PER_CANDIDATE` (in `meco.workflow`): iterations per candidate for metric averaging.
- `DEFAULT_MAX_DEPTH` / `MIN_IMPROVEMENT` (in `meco.workflow`): search depth and epsilon convergence.
- `CodeOptimizer(model=...)`: override OpenAI model, defaults to `gpt-4o-mini`.

## Notes
- Tests run inside a lightweight `python:3.9-slim` container; code and tests are streamed in on every evaluation.
- Ensure generated tests import the function from `solutions.py` (handled automatically when generated by MECO).
